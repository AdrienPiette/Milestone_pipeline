{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing json file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# preprocess json file for nlp\n",
    "def json_to_text(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Transform JSON to text\n",
    "    texts = []\n",
    "    for article in data:\n",
    "        text = (\n",
    "            f\"ID: {article['id']}\\n\"\n",
    "            f\"Slug: {article['slug']}\\n\"\n",
    "            f\"Title: {article['title']}\\n\"\n",
    "            f\"Type: {article['type']}\\n\"\n",
    "            f\"Dossier: {article['dossierLabel']}\\n\"\n",
    "            f\"Summary: {article['summary']}\\n\"\n",
    "            f\"Reading Time: {article['readingTime']} min\\n\"\n",
    "            f\"Published: {article['publishedFrom']}\\n\"\n",
    "            f\"Redaction Info: {article['redactedByTeamRedactionInfo']}\\n\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    # Join all article texts into a single string\n",
    "    global_text = \"\\n\\n---\\n\\n\".join(texts)\n",
    "    return global_text\n",
    "\n",
    "global_text = json_to_text(\"data.json\")\n",
    "\n",
    "# save global_text in .txt file\n",
    "with open(\"global_text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(global_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text Basic Text Cleaning and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# Load the spaCy model\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "nlp.max_length = 5000000\n",
    "\n",
    "# Load the text\n",
    "with open(\"global_text.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 1. Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# 2. Remove HTML tags\n",
    "text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# 3. Remove URLs and email addresses\n",
    "text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "# 4. Remove punctuation and special characters\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# 5. NLP processing with spaCy for tokenization, stop word removal, lemmatization\n",
    "doc = nlp(text)\n",
    "tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "\n",
    "# 7. Remove rare or too frequent words\n",
    "word_freq = Counter(tokens)\n",
    "tokens = [token for token in tokens if 1 < word_freq[token] < 1000]  # Example filtering\n",
    "\n",
    "# Reconstruct the cleaned text\n",
    "cleaned_text = ' '.join(tokens)\n",
    "\n",
    "print(cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging, chunking and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the cleaned_text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Step 1: Part-of-Speech Tagging\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Tag: {token.tag_}\")\n",
    "\n",
    "# Step 2: Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
    "print(\"Lemmas:\", lemmas)\n",
    "\n",
    "# Step 3: Chunking (Noun Phrase Extraction)\n",
    "noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "print(\"Noun Chunks:\", noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
